{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6f3cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAgentic RAG (Retrieval-Augmented Generation) System\\n==================================================\\n\\nThis system demonstrates how to build an intelligent RAG agent that can:\\n1. Decide whether to use retrieval tools or answer directly\\n2. Retrieve relevant documents from a vector database\\n3. Grade document relevance to queries\\n4. Rewrite queries if documents aren't relevant\\n5. Generate final answers using retrieved context\\n\\nThe system uses LangGraph to orchestrate the workflow between different components.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Agentic RAG (Retrieval-Augmented Generation) System\n",
    "==================================================\n",
    "\n",
    "This system demonstrates how to build an intelligent RAG agent that can:\n",
    "1. Decide whether to use retrieval tools or answer directly\n",
    "2. Retrieve relevant documents from a vector database\n",
    "3. Grade document relevance to queries\n",
    "4. Rewrite queries if documents aren't relevant\n",
    "5. Generate final answers using retrieved context\n",
    "\n",
    "The system uses LangGraph to orchestrate the workflow between different components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6a204",
   "metadata": {},
   "source": [
    "#### ENVIRONMENT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d614260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Load environment variables and suppress warnings for cleaner output\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204e6e6",
   "metadata": {},
   "source": [
    "#### CORE COMPONENTS INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bcf25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core LLM, embedding models, and web search tool initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize the language model for reasoning and generation\n",
    "llm = ChatOpenAI(temperature=0)  # Temperature=0 for consistent outputs\n",
    "\n",
    "# Initialize embeddings model for vector similarity search\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize web search tool for fallback when documents are irrelevant\n",
    "web_search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "print(\"Core LLM, embedding models, and web search tool initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea733eb",
   "metadata": {},
   "source": [
    "#### DOCUMENT LOADING & PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13690063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_process_documents():\n",
    "    \"\"\"\n",
    "    Load documents from web sources and split them into chunks for vector storage.\n",
    "    \n",
    "    This function demonstrates the document ingestion pipeline:\n",
    "    1. Load web content using WebBaseLoader\n",
    "    2. Split documents into manageable chunks\n",
    "    3. Prepare documents for embedding and storage\n",
    "    \"\"\"\n",
    "    # Define source URLs - using Lilian Weng's blog posts about AI agents\n",
    "    urls = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
    "    ]\n",
    "    \n",
    "    # Load documents from each URL\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    \n",
    "    # Flatten the list of document lists into a single list\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    \n",
    "    # Split documents into chunks for better retrieval\n",
    "    # Using tiktoken encoder to count tokens accurately\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=100,    # Small chunks for precise retrieval\n",
    "        chunk_overlap=25   # Overlap to maintain context between chunks\n",
    "    )\n",
    "    \n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    \n",
    "    print(f\"Loaded and split {len(doc_splits)} document chunks\")\n",
    "    return doc_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18ee18",
   "metadata": {},
   "source": [
    "#### VECTOR STORE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ae396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "def setup_vector_store_and_retriever(doc_splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks and set up retrieval tool.\n",
    "    \n",
    "    This demonstrates the RAG retrieval component:\n",
    "    1. Create embeddings for all document chunks\n",
    "    2. Store embeddings in Chroma vector database\n",
    "    3. Create a retriever interface\n",
    "    4. Wrap retriever in a tool for agent use\n",
    "    \"\"\"\n",
    "    # Create vector store with embedded documents\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Create retriever interface\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Wrap retriever in a tool that the agent can use\n",
    "    retriever_tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"retriever_blog_post\",\n",
    "        \"\"\"Search and return information about Lilian Weng blog posts on LLM agents, \n",
    "        prompt engineering, and adversarial attacks on LLMs. Use this tool when queries \n",
    "        relate to AI agents, planning, reflection, or prompt engineering concepts.\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store and retriever tool created\")\n",
    "    return [retriever_tool]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a9569",
   "metadata": {},
   "source": [
    "#### AGENT STATE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6782a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Defines the state structure for our agentic workflow.\n",
    "    \n",
    "    The state contains:\n",
    "    - messages: A sequence of messages that gets updated as the agent processes\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c6578",
   "metadata": {},
   "source": [
    "#### AGENT NODES (WORKFLOW COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b225be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_decision_maker(state: AgentState):\n",
    "    \"\"\"\n",
    "    DECISION NODE: Determines whether to use tools or respond directly.\n",
    "    \n",
    "    This is the \"brain\" of the agentic system that decides:\n",
    "    - Should I use the retrieval tool for this query?\n",
    "    - Or can I respond directly without external knowledge?\n",
    "    \n",
    "    The LLM is bound with tools, allowing it to decide when to call them.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Bind tools to LLM so it can decide when to use them\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # Get the most recent message (user query)\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Let LLM decide whether to use tools or respond directly\n",
    "    response = llm_with_tools.invoke(last_message.content)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def grade_documents_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    EVALUATION NODE: Grades retrieved documents for relevance and stores result.\n",
    "    \n",
    "    This function demonstrates document relevance evaluation:\n",
    "    1. Extracts the original question and retrieved documents\n",
    "    2. Uses structured output to get a binary relevance score\n",
    "    3. Adds the relevance score to the state for routing decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    from pydantic import BaseModel, Field\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    # Define structured output format for grading\n",
    "    class DocumentGrade(BaseModel):\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "    # Create LLM with structured output\n",
    "    llm_with_structure = llm.with_structured_output(DocumentGrade)\n",
    "    \n",
    "    # Grading prompt\n",
    "    grading_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing document relevance to a user question.\n",
    "        \n",
    "        Document: {context}\n",
    "        Question: {question}\n",
    "        \n",
    "        If the document contains information relevant to answering the question, grade it as relevant.\n",
    "        Give a binary score: 'yes' for relevant, 'no' for not relevant.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Create grading chain\n",
    "    grading_chain = grading_prompt | llm_with_structure\n",
    "    \n",
    "    # Extract question and documents from state\n",
    "    messages = state['messages']\n",
    "    question = messages[0].content  # Original question\n",
    "    last_message = messages[-1]     # Retrieved documents\n",
    "    docs = last_message.content\n",
    "    \n",
    "    # Grade the documents\n",
    "    scored_result = grading_chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "    # Add grading result to messages for routing\n",
    "    grade_message = HumanMessage(content=f\"GRADE_RESULT:{score}\")\n",
    "\n",
    "    return {\"messages\": [grade_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac2abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    WEB SEARCH NODE: Searches the web when local documents are insufficient.\n",
    "    \n",
    "    This function provides a fallback mechanism:\n",
    "    1. Takes the rewritten query from the previous step\n",
    "    2. Searches the web for relevant information\n",
    "    3. Returns web search results as context for answer generation\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get the most recent query (could be original or rewritten)\n",
    "    query = None\n",
    "    for msg in reversed(messages):\n",
    "        if hasattr(msg, 'content') and not msg.content.startswith(\"GRADE_RESULT:\"):\n",
    "            # Check if this is a rewritten query or the original question\n",
    "            if \"improved question:\" in msg.content.lower() or \"reformulated\" in msg.content.lower():\n",
    "                # Extract the improved question from the rewrite response\n",
    "                content = msg.content\n",
    "                lines = content.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.strip() and not any(skip in line.lower() for skip in ['original', 'consider', 'analyze']):\n",
    "                        query = line.strip()\n",
    "                        break\n",
    "                break\n",
    "            elif msg == messages[0]:  # Original question\n",
    "                query = msg.content\n",
    "                break\n",
    "    \n",
    "    if not query:\n",
    "        query = messages[0].content  # Fallback to original question\n",
    "    \n",
    "    print(f\"Searching for: {query}\")\n",
    "    \n",
    "    try:\n",
    "        # Perform web search\n",
    "        search_results = web_search_tool.run(query)\n",
    "        \n",
    "        # Create a formatted response with search results\n",
    "        web_context = f\"Web Search Results for: {query}\\n\\n{search_results}\"\n",
    "        search_message = HumanMessage(content=web_context)\n",
    "        \n",
    "        print(\"Web search completed successfully\")\n",
    "        return {\"messages\": [search_message]}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Web search failed: {e}\")\n",
    "        # Return a fallback message\n",
    "        fallback_message = HumanMessage(content=f\"Web search unavailable. Using general knowledge to answer: {query}\")\n",
    "        return {\"messages\": [fallback_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3bdeb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_grading(state: AgentState):\n",
    "    \"\"\"\n",
    "    ROUTING FUNCTION: Routes based on document grading results.\n",
    "    \n",
    "    This function checks the grading result and routes accordingly:\n",
    "    - If documents are relevant: go to answer generation\n",
    "    - If documents are not relevant: go to query rewriting\n",
    "    \"\"\"\n",
    "    from typing import Literal\n",
    "    \n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message contains grading result\n",
    "    if hasattr(last_message, 'content') and last_message.content.startswith(\"GRADE_RESULT:\"):\n",
    "        grade = last_message.content.split(\":\")[1]\n",
    "        if grade == \"yes\":\n",
    "            return \"answer_generator\"\n",
    "        else:\n",
    "            return \"query_rewriter\"\n",
    "    \n",
    "    # Default routing if no grade found\n",
    "    return \"answer_generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94363f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: AgentState):\n",
    "    \"\"\"\n",
    "    GENERATION NODE: Creates final answer using retrieved context or web search results.\n",
    "    \n",
    "    This is the final step in successful RAG:\n",
    "    1. Takes the original question and available context (documents or web search)\n",
    "    2. Uses a RAG prompt to generate a contextual answer\n",
    "    3. Returns the generated response\n",
    "    \"\"\"\n",
    "    \n",
    "    from langchain import hub\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content    # Original question\n",
    "    \n",
    "    # Find the most recent context (either retrieved documents or web search results)\n",
    "    context = None\n",
    "    context_type = \"documents\"\n",
    "    \n",
    "    # Look for context in reverse order (most recent first)\n",
    "    for msg in reversed(messages):\n",
    "        if hasattr(msg, 'content') and not msg.content.startswith(\"GRADE_RESULT:\"):\n",
    "            if msg != messages[0]:  # Not the original question\n",
    "                context = msg.content\n",
    "                # Determine if this is web search results or document retrieval\n",
    "                if \"Web Search Results\" in context:\n",
    "                    context_type = \"web_search\"\n",
    "                break\n",
    "    \n",
    "    if context is None:\n",
    "        print(\"No context found, generating response from general knowledge\")\n",
    "        context = \"No specific context available.\"\n",
    "\n",
    "    print(f\"Using {context_type} as context source\")\n",
    "\n",
    "    # Use pre-built RAG prompt from LangChain hub\n",
    "    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    # Create RAG chain\n",
    "    rag_chain = rag_prompt | llm\n",
    "    \n",
    "    # Generate answer using context\n",
    "    response = rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    print(\"Generated contextual answer\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc578cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: AgentState):\n",
    "    \"\"\"\n",
    "    QUERY REWRITING NODE: Improves query when documents aren't relevant.\n",
    "    \n",
    "    This demonstrates query optimization:\n",
    "    1. Analyzes why the original query might not have retrieved good results\n",
    "    2. Reformulates the query to be more specific or use different keywords\n",
    "    3. Sends the improved query back for another retrieval attempt\n",
    "    \"\"\"\n",
    "    print(\"Query Rewriter: Reformulating query for better retrieval...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    \n",
    "    # Query rewriting prompt\n",
    "    rewrite_input = [HumanMessage(content=f\"\"\"\n",
    "    Analyze this question and rewrite it to be more specific and likely to retrieve relevant information:\n",
    "    \n",
    "    Original question: {question}\n",
    "    \n",
    "    Consider:\n",
    "    - What are the key concepts that should be searched for?\n",
    "    - Are there more specific terms or technical language that should be used?\n",
    "    - How can the question be reformulated for better document matching?\n",
    "    \n",
    "    Provide an improved version of the question:\n",
    "    \"\"\")]\n",
    "    \n",
    "    response = llm.invoke(rewrite_input)\n",
    "\n",
    "    print(\"Query rewritten for better retrieval\")\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af25e7",
   "metadata": {},
   "source": [
    "#### WORKFLOW ORCHESTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb460061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "def create_agentic_rag_workflow(tools):\n",
    "    \"\"\"\n",
    "    Creates the agentic RAG workflow using LangGraph.\n",
    "    \n",
    "    This function demonstrates workflow orchestration:\n",
    "    1. Defines all nodes (processing steps)\n",
    "    2. Sets up conditional edges for dynamic routing\n",
    "    3. Creates a state machine that can handle different query types\n",
    "    \n",
    "    Workflow Flow:\n",
    "    START → LLM Decision Maker → [Tools OR End]\n",
    "    Tools → Document Grader → [Generator OR Rewriter]\n",
    "    Generator → END\n",
    "    Rewriter → Web Search → Generator → END\n",
    "    \"\"\"\n",
    "    print(\"Building agentic RAG workflow...\")\n",
    "    \n",
    "    # Create the workflow graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add all processing nodes\n",
    "    workflow.add_node(\"llm_decision_maker\", llm_decision_maker)\n",
    "    workflow.add_node(\"vector_retriever\", ToolNode(tools))\n",
    "    workflow.add_node(\"document_grader\", grade_documents_node)\n",
    "    workflow.add_node(\"answer_generator\", generate_answer)\n",
    "    workflow.add_node(\"query_rewriter\", rewrite_query)\n",
    "    workflow.add_node(\"web_search\", web_search_node)  # New web search node\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.add_edge(START, \"llm_decision_maker\")\n",
    "    \n",
    "    # Decision maker routes to tools OR ends directly\n",
    "    workflow.add_conditional_edges(\n",
    "        \"llm_decision_maker\",\n",
    "        tools_condition,  # Built-in function that checks if tools should be called\n",
    "        {\n",
    "            \"tools\": \"vector_retriever\",  # Use retrieval if tools are needed\n",
    "            END: END                      # End if no tools needed\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After retrieval, grade documents for relevance\n",
    "    workflow.add_edge(\"vector_retriever\", \"document_grader\")\n",
    "    \n",
    "    # Document grader routes to generation OR query rewriting\n",
    "    workflow.add_conditional_edges(\n",
    "        \"document_grader\",\n",
    "        route_after_grading,  # This function returns the routing decision\n",
    "        {\n",
    "            \"answer_generator\": \"answer_generator\",  # Generate if docs are relevant\n",
    "            \"query_rewriter\": \"query_rewriter\"       # Rewrite if docs aren't relevant\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Answer generator ends the workflow\n",
    "    workflow.add_edge(\"answer_generator\", END)\n",
    "    \n",
    "    # Query rewriter goes to web search (following the diagram)\n",
    "    workflow.add_edge(\"query_rewriter\", \"web_search\")\n",
    "    \n",
    "    # Web search results go to answer generator\n",
    "    workflow.add_edge(\"web_search\", \"answer_generator\")\n",
    "\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2c5fc",
   "metadata": {},
   "source": [
    "#### SYSTEM INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27cf7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_agentic_rag_system():\n",
    "    \"\"\"\n",
    "    Initialize the complete agentic RAG system.\n",
    "    \n",
    "    This function ties everything together:\n",
    "    1. Loads and processes documents\n",
    "    2. Sets up vector store and retrieval\n",
    "    3. Creates the workflow\n",
    "    4. Returns ready-to-use RAG agent\n",
    "    \"\"\"\n",
    "    print(\"Initializing Agentic RAG System...\")\n",
    "    \n",
    "    # Step 1: Load and process documents\n",
    "    doc_splits = load_and_process_documents()\n",
    "    \n",
    "    # Step 2: Set up vector store and retriever\n",
    "    global tools  # Make tools global so other functions can access\n",
    "    tools = setup_vector_store_and_retriever(doc_splits)\n",
    "    \n",
    "    # Step 3: Create workflow\n",
    "    app = create_agentic_rag_workflow(tools)\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab03ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_system_capabilities(app):\n",
    "    \"\"\"\n",
    "    Demonstrate different capabilities of the agentic RAG system.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test cases demonstrating different system behaviors\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Complex Technical Query (Uses RAG)\",\n",
    "            \"query\": \"What is LLM Powered Autonomous Agents? Explain planning and reflection in terms of agents and LangChain.\",\n",
    "            \"expected\": \"Should use retrieval tool and generate contextual answer\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Simple Greeting (Direct Response)\",\n",
    "            \"query\": \"Hi, how are you?\",\n",
    "            \"expected\": \"Should respond directly without using tools\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Specific Concept Query (Uses RAG)\",\n",
    "            \"query\": \"Can you explain task decomposition and why Chain of Thought (CoT) prompting enhances model performance?\",\n",
    "            \"expected\": \"Should retrieve relevant documents and generate detailed answer\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- Test Case {i}: {test_case['name']} ---\")\n",
    "        print(f\"Query: {test_case['query']}\")\n",
    "        print(f\"Expected: {test_case['expected']}\")\n",
    "        print(\"\\nResponse:\")\n",
    "        \n",
    "        try:\n",
    "            result = app.invoke({\"messages\": [test_case['query']]})\n",
    "            # Extract the final message content\n",
    "            final_message = result['messages'][-1].content\n",
    "            print(f\"{final_message[:200]}...\" if len(final_message) > 200 else f\" {final_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bdc89c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agentic RAG System...\n",
      "Loaded and split 287 document chunks\n",
      "Vector store and retriever tool created\n",
      "Building agentic RAG workflow...\n",
      "\n",
      "--- Test Case 1: Complex Technical Query (Uses RAG) ---\n",
      "Query: What is LLM Powered Autonomous Agents? Explain planning and reflection in terms of agents and LangChain.\n",
      "Expected: Should use retrieval tool and generate contextual answer\n",
      "\n",
      "Response:\n",
      "Query Rewriter: Reformulating query for better retrieval...\n",
      "Query rewritten for better retrieval\n",
      "Searching for: Improved question: Can you provide an explanation of how LLM Powered Autonomous Agents utilize planning and reflection within the context of LangChain technology?\n",
      "Web search completed successfully\n",
      "Using web_search as context source\n",
      "Generated contextual answer\n",
      "LLM Powered Autonomous Agents are software applications that use large language models to perform tasks independently. Planning and reflection in terms of agents and LangChain involve leveraging natur...\n",
      "\n",
      "--- Test Case 2: Simple Greeting (Direct Response) ---\n",
      "Query: Hi, how are you?\n",
      "Expected: Should respond directly without using tools\n",
      "\n",
      "Response:\n",
      " Hello! I'm here and ready to assist you. How can I help you today?\n",
      "\n",
      "--- Test Case 3: Specific Concept Query (Uses RAG) ---\n",
      "Query: Can you explain task decomposition and why Chain of Thought (CoT) prompting enhances model performance?\n",
      "Expected: Should retrieve relevant documents and generate detailed answer\n",
      "\n",
      "Response:\n",
      "Using documents as context source\n",
      "Generated contextual answer\n",
      "Task decomposition involves breaking down complex tasks into smaller and simpler steps. Chain of Thought (CoT) prompting enhances model performance by instructing the model to \"think step by step,\" al...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the system\n",
    "    rag_agent = initialize_agentic_rag_system()\n",
    "    \n",
    "    # Demonstrate capabilities\n",
    "    demonstrate_system_capabilities(rag_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_2_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
